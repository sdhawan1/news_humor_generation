{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CBOW.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0l6ryNnA8V1l","outputId":"4c68fd5b-9ef8-462c-8b2c-44bc9ba8abf2"},"source":["'''\n","  Here, we will try to train a CBOW model on news articles and Tweets.\n","  It should be able to find words that fit into a particular news headline.\n","\n","\n","  A couple of different options for datasets:\n","    1. https://www.kaggle.com/rmisra/news-category-dataset\n","      * This dataset contains a bunch of politics (and other subject) - related news headlines.\n","      ~ Let's maybe start with this one.\n","\n","    2. https://www.kaggle.com/snapcrack/all-the-news\n","      * This dataset contains full news articles; you can separate them by publication.\n","      * In particular, I think it might be a good idea to train on Brietbart and have the model spew a bunch of ridiculous garbage.\n","      * Though remember, we are only filling in one word...\n","  \n","\n","\n","'''\n","\n","\n","!wget https://www.kaggle.com/rmisra/news-category-dataset/download\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-12-02 10:24:50--  https://www.kaggle.com/rmisra/news-category-dataset/download\n","Resolving www.kaggle.com (www.kaggle.com)... 35.244.233.98\n","Connecting to www.kaggle.com (www.kaggle.com)|35.244.233.98|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /account/login?titleType=dataset-downloads&showDatasetDownloadSkip=False&messageId=16&returnUrl=%2Frmisra%2Fnews-category-dataset%3Fresource%3Ddownload [following]\n","--2020-12-02 10:24:51--  https://www.kaggle.com/account/login?titleType=dataset-downloads&showDatasetDownloadSkip=False&messageId=16&returnUrl=%2Frmisra%2Fnews-category-dataset%3Fresource%3Ddownload\n","Reusing existing connection to www.kaggle.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘download’\n","\n","download                [ <=>                ]   7.61K  --.-KB/s    in 0.04s   \n","\n","2020-12-02 10:24:51 (211 KB/s) - ‘download’ saved [7793]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3a1DXp2H-NRy","outputId":"abb62866-0d17-4dd9-c361-e3c9a2f3429f"},"source":["#mount google drive\n","\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ciVxHjr1y7Cv","outputId":"d9080ea3-d590-4d80-c7c5-ac29229777c4"},"source":["!ls '/content/gdrive/MyDrive/humor_generation'\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/humor_generation')\n","\n","from cbow_helper import *"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cbow_helper.py\tcbow_news  pungen-master\t skipgram_surprise.ipynb\n","CBOW.ipynb\tdatasets   silly_synonyms.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GM2fna-YVlu_"},"source":["# Preprocessing 1:\n","\n","Let's read in the headline data & organize it.\n"]},{"cell_type":"code","metadata":{"id":"q9RZGv2tACfS"},"source":["# first, let's extract all the headlines\n","import json\n","\n","pth = 'gdrive/MyDrive/humor_generation/datasets/News_Category_Dataset_v2.json'\n","\n","hl_list = []\n","ctr = 0\n","with open(pth) as f:\n","  for line in f:\n","    # print(line)\n","    j = json.loads(line)\n","    if ctr < 10:\n","      print(j['category'])\n","      print(j)\n","      ctr += 1\n","    #break\n","    hl_list.append(j)\n","\n","\n","# single out the headline texts from political headlines.\n","print(\"\\n\\nIsolating Politics Headline text...\")\n","politics_headlines = [j['headline'] for j in hl_list if j['category'] == 'POLITICS']\n","print(politics_headlines[:5])\n","print(len(politics_headlines)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TVk3c1ZzV8o7"},"source":["# Preprocessing 2:\n","\n","Tokenize and prepare the data to start training CBOW"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pVBbAt5MsEvv","outputId":"8fa9bd9e-50ad-48ce-9ec8-2f42ab6b4159"},"source":["# STEP 1\n","# tokenize the training data, and create an index of word ids-> words, as well as words -> word id's.\n","# inputs: \"trdata\": needs to be organized as a list of sentences included in the corpus\n","#         \"trdata_dir\": will contain indices & other stuff for the training data.\n","\n","from keras.preprocessing import text\n","import pickle as pkl\n","import numpy as np\n","\n","import sys\n","pth = '/content/gdrive/MyDrive/humor_generation'\n","if not (pth in sys.path):\n","  sys.path.append(pth)\n","from cbow_helper import cbow\n","\n","def tokenize_trdata(trdata, trdata_dir):\n","  tokenizer = text.Tokenizer()\n","  tokenizer.fit_on_texts(trdata)  # this just needs a list of strings\n","  word2id = tokenizer.word_index\n","  # add padding...\n","  word2id['PAD'] = 0\n","\n","  vocab_size = len(word2id)\n","  # create id2word & word2id; save both.\n","  id2word = {v:k for k, v in word2id.items()}\n","\n","  # save word2id and id2word arrays as pickles. You will need them for later...\n","  with open('{}/word2id.pkl'.format(trdata_dir), 'wb') as fout:\n","    pkl.dump(word2id, fout)\n","  with open('{}/id2word.pkl'.format(trdata_dir), 'wb') as fout:\n","    pkl.dump(id2word, fout)\n","\n","  # convert the corpus to a set of IDs.\n","  wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in politics_headlines]\n","\n","  print(\"vocab size:\", vocab_size)\n","  print(\"vocab items:\", list(word2id.items())[:10] )\n","\n","  return wids, word2id, id2word\n","\n","\n","\n","# STEP 2\n","# convert tokenized dataset into tokenized positive examples, to train cbow.\n","# come up with a list of pairs (center_wd, [context_wds]). EPredict center word from context.\n","\n","# inputs: \n","#     trdata_tokenized: all the sentences need to be converted to lists of numerical tokens.\n","#     trdata & trdata_dir: same as step 1\n","# outputs:\n","#     outfile: will extract and store all the examples like: np.array([iword, *owords]), where iword is tokenized center word, and oword is the list of tokenized context wds.\n","\n","def tokens_to_nn_input(trdata_dir, trdata, trdata_tokenized, outfile = 'train.bin'):\n","\n","  print(\"converting corpus ...\")\n","  step = 0\n","  fout = open('{}/{}'.format(trdata_dir, outfile), 'wb')\n","  for step, line in enumerate(trdata_tokenized):\n","    if not step % 1000:\n","      print(\"working on {}kth line\".format(step // 1000), end='\\r')\n","\n","    #[DEBUG] make sure we are iterating through the right stuff here...\n","    if step < 10:\n","      print(\"word ids: \", step)\n","      print(\"orig. sentence: \", politics_headlines[step])\n","    \n","    sent = line\n","    # [edit] I may not need this...\n","    # if len(sent) <= window_size:\n","    #  continue\n","    for i in range(len(sent)):\n","      iword, owords = cbow(sent, i, window_size)\n","      a = np.array([iword] + owords, dtype=np.uint16)\n","      #debug\n","      if step < 10:\n","        print(a)\n","      fout.write(a.tobytes())\n","\n","  fout.close()\n","  print(\"conversion done\")\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vocab size: 23187\n","vocab items: [('to', 1), ('the', 2), ('trump', 3), ('of', 4), ('in', 5), ('for', 6), ('a', 7), ('on', 8), ('is', 9), ('donald', 10)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1BvKSWgTufaq"},"source":["## actually run \"preprocessing step 1\"\n","\n","# run the above function with the given parameters.\n","trdata = politics_headlines\n","trdata_dir = 'gdrive/MyDrive/humor_generation/cbow_news'\n","wids, word2id, id2word = tokenize_trdata(trdata, trdata_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"saoHyUFYLkMG"},"source":["## actually run \"preprocessing step 2\"\n","\n","window_size = 3  # try making this bigger?\n","tokens_to_nn_input(trdata_dir, trdata, wids, window_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6cVVJvFPWewL"},"source":["# Start Training CBOW\n","\n","Edit: Added GPU's & Batched GD. \\{concern: may need to tune LR\\}\n","\n","\n","Train on the output of preprocessing steps"]},{"cell_type":"code","metadata":{"id":"VX8vnRGDLK6C"},"source":["# NOTE: before starting on this, you can either preprocess using the above functions, or load results from pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9v5e7TRbpa-","outputId":"b51cc6a8-560a-4ebd-e34d-337614633cb3"},"source":["## specify hardware.\n","import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lnMq2VdKa-_-"},"source":["# goal of this cell: design training code that can run on gpu\n","# TODO: Verify training accuracy with some kind of \"validation loss\"\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","import sys\n","pth = '/content/gdrive/MyDrive/humor_generation'\n","if not (pth in sys.path):\n","  sys.path.append(pth)\n","from cbow_helper import CBOWLanguageModeler\n","\n","torch.manual_seed(1)\n","\n","# this class should take a 2-d numpy array and return one row at a time.\n","class CBOWDataset(torch.utils.data.Dataset):\n","  def __init__(self, np_2d_array):\n","    super(CBOWDataset).__init__()\n","    self.data = np_2d_array\n","  def __getitem__(self, index):\n","    return self.data[index]\n","  def __len__(self):\n","    return self.data.shape[0]\n","\n","\n","# function to save the checkpoint + other metadata.\n","def save_ckp(ckp_path, epoch, losses, model, optimizer, \n","             VOCAB_SIZE, CONTEXT_SIZE, batch_size, EMBEDDING_DIM):\n","  checkpoint = {\n","    'epoch': epoch,\n","    'losses': losses,\n","    'state_dict': model.state_dict(),\n","    'optimizer': optimizer.state_dict(),\n","    # these are params for model initialization\n","    'vocab_size': VOCAB_SIZE,\n","    'context_size': CONTEXT_SIZE,\n","    'batch_size': batch_size,\n","    'embedding_dim': EMBEDDING_DIM\n","  }\n","  torch.save(checkpoint, ckp_path)\n","\n","\n","# actually start up the training process\n","def train_cbow(trdata_dir, VOCAB_SIZE, trdata_filename = 'train.bin', window_size = 3, \n","               EMBEDDING_DIM = 100, batch_size=10, lr = 0.001, ckpt_start_path = None):\n","  epoch = -1\n","\n","  # first, pull up the training data.\n","  context_size = 2*window_size + 1  # 2 context n-grams on each side, plus the center word\n","  fin = open(trdata_dir + trdata_filename, 'rb')\n","  data = np.fromfile(fin, dtype=np.uint16, count=-1)  # note: this produces a 1-d array.\n","  data = data.astype('int16')\n","  data = data.reshape((-1,context_size))\n","\n","  # then, create a batch data loader.\n","  batched_data = DataLoader(CBOWDataset(data), batch_size)\n","\n","\n","  # finally, it's time to perform the training... Initialize the model.\n","  losses = []\n","  loss_function = nn.NLLLoss()\n","  CONTEXT_SIZE = 2*window_size\n","  model = CBOWLanguageModeler(VOCAB_SIZE, CONTEXT_SIZE, batch_size, EMBEDDING_DIM)\n","  optimizer = optim.SGD(model.parameters(), lr=lr)  # TODO: update this!! [don't use sgd]\n","\n","  # if there is a saved model & optimizer state, load those up.\n","  if ckpt_start_path is not None:\n","    checkpoint = torch.load(ckpt_start_path)\n","    epoch = checkpoint['epoch']-1\n","    losses = checkpoint['losses']\n","    model_sd = checkpoint['state_dict']\n","    optimizer_sd = checkpoint['optimizer']\n","    model.load_state_dict(model_sd)\n","    optimizer.load_state_dict(optimizer_sd)\n","\n","  #put this on the gpu\n","  model.to(device)\n","\n","  # now we have to cycle through the training data\n","  ## note: adding batched processing.\n","  print(\"starting training process...\")\n","  print(\"batch size:\", batch_size)\n","  while epoch < 10:\n","    epoch += 1\n","\n","    # print training progress...\n","    if epoch > 0:\n","      print(f\"total loss for epoch #{epoch-1}: {total_loss}\")\n","    print(\"\\n#--------------------------------------------------------------#\")\n","    print(\"\\tstarting epoch #\", epoch)\n","\n","    total_loss = 0\n","\n","    #for context, target in trigrams:\n","    for step, context_row in enumerate(batched_data):\n","      niter = int(1000/batch_size)\n","      if not step % niter:\n","        print(f\"working on step #{step} of {data.shape[0]/batch_size}\")\n","      center_wd = context_row[:,0]\n","      context = context_row[:,1:]\n","\n","      # save the model. [save after 50k examples.]\n","      if step == int(50000/batch_size):\n","        print(\"saving model...\")\n","        save_ckp(f'gdrive/MyDrive/humor_generation/cbow_news/checkpoint_{epoch}_{step}.pt',\n","         epoch, losses, model, optimizer,\n","         VOCAB_SIZE, CONTEXT_SIZE, batch_size, EMBEDDING_DIM) #these are the arguments to initialize the model.        \n","        # [DEBUG]: REMOVE THIS STATEMENT LATER!\n","        # break\n","\n","      \n","      # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n","      # into integer indices and wrap them in tensors)\n","      # [=== old ===] # context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n","      context_idxs = torch.tensor(context, dtype=torch.long).to(device)\n","\n","      # Step 2. reset the gradients.\n","      model.zero_grad()\n","\n","      # Step 3. Run the forward pass\n","      log_probs = model(context_idxs)\n","\n","      # Step 4. Compute your loss function. (Again, Torch wants the target\n","      # word wrapped in a tensor)\n","      center_wd_tsr = torch.tensor(center_wd, dtype=torch.long)\n","      loss = loss_function(log_probs.to('cpu'), center_wd_tsr)\n","\n","      # Step 5. Do the backward pass and update the gradient\n","      loss.backward()\n","      optimizer.step()\n","\n","      # Get the Python number from a 1-element Tensor by calling tensor.item()\n","      total_loss += loss.item()\n","      losses.append(total_loss)\n","\n","  print(losses)  # The loss decreased every iteration over the training data!\n","  return model, losses\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRARDCTKyD_x"},"source":["\n","#-----------------------------------------------------------------------------#\n","### actually run the function\n","#-----------------------------------------------------------------------------#\n","\n","trdata_dir = 'gdrive/MyDrive/humor_generation/cbow_news/'\n","trdata_filename = 'train.bin'\n","window_size = 3\n","EMBEDDING_DIM = 100\n","VOCAB_SIZE = 23187  # 23187 # len(word2id)\n","lr = 0.001\n","\n","model, losses = train_cbow(trdata_dir, VOCAB_SIZE, trdata_filename, window_size, EMBEDDING_DIM)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RoMrlx6BjyvP"},"source":["Using these two articles to learn:\n","1. https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html\n","\n","2. https://www.guru99.com/word-embedding-word2vec.html"]},{"cell_type":"markdown","metadata":{"id":"SdCA6pLdxQaA"},"source":["# CBOW Inference\n","\n","The model trained in the previous cells needs to be applied as part of the autmentation pipeline (using an inference script).\n","* Run it on training data and News Headlines data.\n","* In the future, create a \"validation set\" from training data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jN5OHsZz6_YB","outputId":"60d52c02-b76a-4025-e720-cad03402b0b1"},"source":["# open up the incomplete news headlines, just for testing purposes.\n","import pandas as pd\n","import pickle as pkl\n","\n","# load the training index pickles, and reconstruct some of the training examples.\n","id2word = pkl.load(open('gdrive/MyDrive/humor_generation/cbow_news/id2word.pkl', 'rb'))\n","word2id = pkl.load(open('gdrive/MyDrive/humor_generation/cbow_news/word2id.pkl', 'rb'))\n","\n","# load the training / test data\n","pth = 'gdrive/MyDrive/humor_generation/datasets/news-headlines-humor-parsed.csv'\n","df = pd.read_csv(pth)\n","\n","print(df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["      id  ...                   verb loc\n","0   1723  ...  start_char=34|end_char=43\n","1  12736  ...  start_char=19|end_char=28\n","2  12274  ...  start_char=15|end_char=21\n","3  12274  ...  start_char=44|end_char=53\n","4  12274  ...  start_char=44|end_char=53\n","\n","[5 rows x 6 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_3PrTR234OOg"},"source":["df['target text'].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ougl_73g7icj"},"source":["# this function actually uses the model to make word predictions based on the context.\n","from keras.preprocessing import text\n","import torch\n","import numpy as np\n","\n","import sys\n","pth = '/content/gdrive/MyDrive/humor_generation'\n","if not (pth in sys.path):\n","  sys.path.append(pth)\n","from cbow_helper import *\n","\n","\n","# load the trained CBOW model (defined in helper file) from model checkpoint file.\n","def load_model(model_path):\n","  # open the checkpoint & load the model.\n","  checkpoint = torch.load(model_path)\n","  epoch = checkpoint['epoch']-1\n","  losses = checkpoint['losses']\n","  model_sd = checkpoint['state_dict']\n","  if checkpoint.get('context_size') is None:\n","    window_size = 3\n","    CONTEXT_SIZE = 2*window_size\n","    EMBEDDING_DIM = 100\n","    VOCAB_SIZE = 23187  # 23187 # len(word2id)\n","    batch_size = 1  # we are going to feed a single example to the model at a time.\n","    lr = 0.001\n","  else:\n","    CONTEXT_SIZE = checkpoint['context_size']\n","    window_size = CONTEXT_SIZE/2\n","    EMBEDDING_DIM = checkpoint['EMBEDDING_DIM']\n","    VOCAB_SIZE = checkpoint['VOCAB_SIZE']\n","    lr = checkpoint['lr']\n","    batch_size = 1  # this I'm maintaining... I want stochastic inference...\n","\n","  # todo in future: save these parameters in the checkpoint?\n","\n","  model = CBOWLanguageModeler(VOCAB_SIZE, CONTEXT_SIZE, batch_size, EMBEDDING_DIM)\n","  model.load_state_dict(model_sd)\n","\n","  print(f\"Extracted model........ epoch {epoch}\")\n","  print(f\"Current training loss: {losses[-1] / VOCAB_SIZE}\")\n","  return model, window_size\n","\n","\n","# helper function: for a given sentence, use the context to find cbow suggestions.\n","# inputs:\n","#     sentence: the actual input sentence to be augmented.\n","#     tgt_idx: the index of the word to augment?\n","#     window_size: number of words in context (on either side)\n","#     model: the trained CBOW model (needs to be loaded first)\n","#     word2id & id2word: indexes for the training dataset.\n","#     threshold: how many alternate words to return\n","# outputs:\n","#     words: list of alternate words that could belong in the sentence.\n","#     (return list of top 10 words, in order, suggested by the model.)\n","def context_alternate_preds(sentence, tgt_idx, window_size, model, word2id, id2word, thresh=10):\n","  # also need to deal with words not in the word index...\n","  sentence = [word2id.get(w) for w in sentence]\n","  sentence = [s if s else np.random.randint(len(word2id)) for s in sentence]  # can replace \"0\" with random integer.\n","  iword, cwords = cbow(sentence, tgt_idx, window_size)\n","  cwords = torch.tensor(cwords)\n","  # print(\"[debug] - cwords:\", cwords)\n","\n","  # send the input to the model, and convert the output to words...\n","  log_probs = model(cwords)  # run fwd prop.\n","  # print(\"examining output...\")\n","  # print(f\"log probs length: {len(log_probs)}\")\n","  # print(f\"log probs content: {log_probs}\")\n","\n","  # pull out the max 5-10 indices and find the actual words.\n","  lp = log_probs.detach().numpy()\n","  winds = np.argsort(lp*-1)[0][:thresh*5]\n","  words = [id2word[i] for i in winds]\n","  #filter out the stopwords & words in the sentence.\n","  print(len(words))\n","  words = list(words)\n","  words = [w for w in words if w not in sw]\n","  words = [w for w in words if w not in sentence]\n","  print(\"[debug] - top words:\", words[:10])\n","\n","  # print(words)\n","  return words[:thresh]\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_dDt9gzM3PYT"},"source":["# In this cell, write a formal script to parse headlines from a file and generate alternate words.\n","import torch\n","import numpy as np\n","import json\n","import pandas as pd\n","import csv\n","\n","import sys\n","pth = '/content/gdrive/MyDrive/humor_generation'\n","if not (pth in sys.path):\n","  sys.path.append(pth)\n","from cbow_helper import CBOWLanguageModeler\n","\n","\n","def augment_sentences(in_path, out_path, model, window_size):\n","  #step 0: load the model # [or just pass it in...]\n","  # model = load_model(model_path)\n","\n","  #step 1: open file, extract headlines\n","  df_news = pd.read_csv(in_path)\n","  ids = df_news['id'].tolist()\n","  sent_list = df_news['sentence'].tolist()\n","  tgt_wds = df_news['target text'].tolist()\n","  vbs = df_news['verb text'].tolist()\n","\n","  #step 2: pass headlines to the generator; save the top 3?\n","  # we need: 1) \"hdl\" variable with sentences; 2) \"tgt_wds\" with target words.\n","  with open(out_path, 'w') as fout:\n","    hdl_writer = csv.writer(fout)\n","    hdl_writer.writerow(['id', 'sentence', 'new_word', 'orig_word', 'target_word_idx' 'target_verb'])\n","    for e, hdl in enumerate(sent_list):\n","      hdl_tokenized = text.text_to_word_sequence(hdl, filters='\\'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True)\n","      tgt = tgt_wds[e].lower()\n","      id = ids[e]\n","      vb = vbs[e]\n","      # isolate the context words & convert to NN input [create new fn. from this?]\n","      if tgt not in hdl_tokenized:\n","        print(f\"skipping, tgt {tgt} & sent {hdl_tokenized}\")\n","        continue\n","      tgt_idx = hdl_tokenized.index(tgt)\n","      alt_words = context_alternate_preds(hdl_tokenized, tgt_idx, window_size, model, word2id, id2word, thresh=10)  # perform NN predictions, thresh=3.\n","      #print(\"headline:\", hdl)\n","      #print(\"target word:\", tgt)\n","      #print(\"top alternate words:\", alt_words)\n","\n","      #create alternate headlines\n","      l = hdl_tokenized[:tgt_idx]\n","      r = hdl_tokenized[(tgt_idx+1):]\n","\n","      # Todo: come up with a proper format and save below to a file.\n","      for w in alt_words:\n","        out_row = [id, ' '.join(l+[w]+r), w, tgt, tgt_idx, vb]  # ['id', 'sentence', 'new_word', 'orig_word', 'target_word_idx' 'target_verb']\n","        if e <= 10:\n","          print(out_row)  # todo: save this output to a file - csv format\n","        hdl_writer.writerow(out_row)\n","\n","\n","#---------------------------------------#\n","# actually run the above function\n","\n","fin = '/content/gdrive/MyDrive/humor_generation/datasets/news-headlines-humor-parsed.csv'\n","fout = '/content/gdrive/MyDrive/humor_generation/cbow_news/in_process_data/alternate_preds.csv'\n","model_path = 'gdrive/MyDrive/humor_generation/cbow_news/checkpoint_8_5000.pt'\n","model, window_size = load_model(model_path)\n","augment_sentences(fin, fout, model, window_size)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ra0mZtmQJx17"},"source":["# \"Scratch\" code"]},{"cell_type":"code","metadata":{"id":"dj4W55Iq-Z-_"},"source":["!ls '/content/gdrive/MyDrive/humor_generation/cbow_news/in_process_data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EInBnMdByyIc"},"source":["# load some examples from the training data, delete their nouns manually, and perform inference\n","# (this is more exploratory, prev. cell will have a more formal script.)\n","'''\n","  Problem with the below:\n","    1. Pull out stopwords\n","    2. Pull out tokens already in the sentence\n","    3. Try training with more data (not just politics headlines), to get a greater variety of words...\n","      ** try training with larger context window... I think this could help...\n","      ** also try lemmatizing and removing stopwords before training?\n","    4. Need to correct grammar\n","\n","'''\n","\n","\n","window_size = 3\n","\n","n_examples = 10\n","hdls = df['Headline'].head(n_examples).tolist()\n","print(hdls)\n","\n","# [singling out words to replace] this will have to be done automatically...\n","tgt_wds = ['prosecutor', 'suspects', 'reporter', 'missile', 'primary', 'scapegoat', 'trolling', 'iran', 'bomb', 'midterms']\n","\n","'''\n","  - This gives me an idea... I'm singling out the words that need to be changed using *attention*\n","  - I should really be training a transformer to find the location of the funniest word...\n","  - Another idea: I think the \"object\" of the verb always tends to be the funniest... Try to single those out as well...\n","'''\n","\n","\n","#todo next: you need to use the same tokenizer used during training...\n","for e, hdl in enumerate(hdls):\n","  print(\"\\n\\n#---------------------------------------#\\n\")\n","  hdl_tokenized = text.text_to_word_sequence(hdl, filters='\\'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True)\n","  tgt = tgt_wds[e]\n","  # isolate the context words & convert to NN input [create new fn from this?]\n","  tgt_idx = hdl_tokenized.index(tgt)\n","  alt_words = context_alternate_preds(hdl_tokenized, tgt_idx, window_size, model, word2id, id2word, thresh=10)\n","  print(\"headline:\", hdl)\n","  print(\"target word:\", tgt)\n","  print(\"top alternate words:\", alt_words)\n","\n","\n","### NEXT MODIFICATION: Need to crawl examples ^^ from a file and run this inference script."],"execution_count":null,"outputs":[]}]}