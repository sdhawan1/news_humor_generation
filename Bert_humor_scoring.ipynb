{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bert_humor_scoring.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e19194e28a5d44e6889efb9e62a8abd5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_89f5953b33514fcd92a8bf8ca5d3f703","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ac37de18aefb4a198562fac8052c822d","IPY_MODEL_c7a9d8651026431290c9f033d3fffe6a"]}},"89f5953b33514fcd92a8bf8ca5d3f703":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac37de18aefb4a198562fac8052c822d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_64132e0c697447a1ba857f6b2d10a5c1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2427deb469de4572b48ecaf249fc8023"}},"c7a9d8651026431290c9f033d3fffe6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_378bf29ed2444d19b35cd462faec69d1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 3.13MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_082d1801649c4e95b932f0ad6112a720"}},"64132e0c697447a1ba857f6b2d10a5c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2427deb469de4572b48ecaf249fc8023":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"378bf29ed2444d19b35cd462faec69d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"082d1801649c4e95b932f0ad6112a720":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8ff4b1e309944c08a526cc4239af0a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4de695b74d2c4a8096e06b9cf49f86da","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8e0661b06ced4c35b40976b69957c59b","IPY_MODEL_c5ec6b6b14b24a5da778d75c2d82dc1a"]}},"4de695b74d2c4a8096e06b9cf49f86da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8e0661b06ced4c35b40976b69957c59b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_afe893dc0e904d00a97862e91e15f0a5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_17f81827dfb14bc582537428ac70cde2"}},"c5ec6b6b14b24a5da778d75c2d82dc1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3edd8c4f97e34d7ea45ffe4410c619c7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 2.12kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3703904f2af64d6692d19ccc96e7c434"}},"afe893dc0e904d00a97862e91e15f0a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"17f81827dfb14bc582537428ac70cde2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3edd8c4f97e34d7ea45ffe4410c619c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3703904f2af64d6692d19ccc96e7c434":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"74a2fb1a96c3442198b276d84e83dd68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3eb9a84db6444e51ae13bb8c719764b6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c583f244b5644e91962b53454ff3405b","IPY_MODEL_fa31687e1e4e43f7a33a380d736cd3b6"]}},"3eb9a84db6444e51ae13bb8c719764b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c583f244b5644e91962b53454ff3405b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dddfba47771c4036b2a97942f083808e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4dbdd7f9dd2a43519f9b12af271a6afa"}},"fa31687e1e4e43f7a33a380d736cd3b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5e12839bf24b485a83375f67dce4cff6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [01:42&lt;00:00, 4.30MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_041dd252ff414b8dbc9ffa67f85b4343"}},"dddfba47771c4036b2a97942f083808e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4dbdd7f9dd2a43519f9b12af271a6afa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5e12839bf24b485a83375f67dce4cff6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"041dd252ff414b8dbc9ffa67f85b4343":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SHwACzBZy8aL"},"source":["# Humor Scoring Using BERT\n","\n","In this notebook, we will train BERT to score input sentences as humorous or non-humorous. It will be trained on the modified headlines dataset.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MSr9Ud_6rb-5","executionInfo":{"status":"ok","timestamp":1608283274252,"user_tz":480,"elapsed":64208,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"09413960-0338-4b54-8756-fce3ca01f92e"},"source":["#mount google drive\n","\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tMzZj2osQXYv"},"source":["## Part 1: Preprocessing\n","\n","Process the input so that it's ready for the training process"]},{"cell_type":"code","metadata":{"id":"YQlg3W3Jzr5o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608283286173,"user_tz":480,"elapsed":6103,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"704a6e69-ab8a-49eb-8151-47e788eacd23"},"source":["### dependencies...\n","\n","#install bert dependency: transformer library\n","!pip install transformers\n","#import transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\r\u001b[K     |▏                               | 10kB 26.2MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 17.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 15.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 14.2MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 11.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 11.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 11.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 12.2MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 11.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 12.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 12.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122kB 12.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133kB 12.5MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 153kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 163kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 174kB 12.5MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 194kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 245kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 256kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 266kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 296kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 307kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 317kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 327kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 337kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 348kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 358kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 368kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 378kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 389kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 399kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 409kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 419kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 440kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 450kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 460kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 471kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 481kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 491kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 501kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 512kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 522kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 532kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 542kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 552kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 563kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 573kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 583kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 593kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 604kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 614kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 624kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 634kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 645kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 655kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 665kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 675kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 686kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 696kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 706kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 716kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 727kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 737kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 747kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 757kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 768kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 778kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 788kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 798kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 808kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 819kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 829kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 839kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 849kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 860kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 870kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 880kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 890kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 901kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 911kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 921kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 931kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 942kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 952kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 962kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 972kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 983kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 993kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 12.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 49.2MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 59.0MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=b1a60559b1e24ccccd60353eebd66c2fc538a04653503f83cdd0c6742cd1150e\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4u8BWYMsEck","executionInfo":{"status":"ok","timestamp":1608283289752,"user_tz":480,"elapsed":523,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"4b3dfbab-35d4-4159-8aec-0d2402357107"},"source":["# check the google drive path\n","!ls 'gdrive/MyDrive/humor_generation/helpers'\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bert_fns.py  prep_humor_data.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mdZK-54O222d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608283300303,"user_tz":480,"elapsed":7272,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"41de8497-a3a5-4551-8178-727959079999"},"source":["# more dependencies...\n","import sys\n","if 'gdrive/MyDrive/humor_generation/helpers' not in sys.path:\n","  sys.path.append('gdrive/MyDrive/humor_generation/helpers')\n","print(sys.path)\n","%run 'gdrive/MyDrive/humor_generation/helpers/prep_humor_data.py'\n","%run 'gdrive/MyDrive/humor_generation/helpers/bert_fns.py'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython', 'gdrive/MyDrive/humor_generation/helpers']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cD2t3NX5ze0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608283306223,"user_tz":480,"elapsed":1747,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"ed30ed8b-a61d-499d-8e54-d8304b88d4b2"},"source":["#preprocess the data... (use inbuilt function)\n","fpath1 = 'gdrive/MyDrive/humor_generation/datasets/bert_training/train.csv'\n","# make a path for the humorous statements that I made.\n","fpath2 = 'gdrive/MyDrive/humor_generation/datasets/bert_training/train_funlines.csv'\n","d1, funny, not_funny = preprocess_data(fpath1)\n","d2, f2, nf2 = preprocess_data(fpath2)\n","data = d1 + d2\n","funny += f2\n","not_funny += nf2\n","\n","#change data format & create train / test sets.\n","print(\"total number of funny instances: \" + str(funny))\n","print(\"total number of non funny instances: \" + str(not_funny))\n","\n","#[handle augmented data here]\n","create_aug_data = False"],"execution_count":null,"outputs":[{"output_type":"stream","text":["9652\n","8248\n","total number of funny instances: 10822\n","total number of non funny instances: 7078\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"19dpp5Fp7kR3","colab":{"base_uri":"https://localhost:8080/","height":416,"referenced_widgets":["e19194e28a5d44e6889efb9e62a8abd5","89f5953b33514fcd92a8bf8ca5d3f703","ac37de18aefb4a198562fac8052c822d","c7a9d8651026431290c9f033d3fffe6a","64132e0c697447a1ba857f6b2d10a5c1","2427deb469de4572b48ecaf249fc8023","378bf29ed2444d19b35cd462faec69d1","082d1801649c4e95b932f0ad6112a720"]},"executionInfo":{"status":"ok","timestamp":1608283325761,"user_tz":480,"elapsed":7214,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"1b6d4e4e-bd83-46df-e9d2-48ba637af3a3"},"source":["#Start with BERT-specific prepping... tokenize input & get attn mask.\n","sentences = [d[0] for d in data]\n","labels = [d[1] for d in data]\n","print(sentences[0])\n","print(labels[0])\n","\n","#use pre-written fn to tokenize the sentences and create attn mask.\n","input_ids, attn_mask = tokenize_inputs_attn_mask(sentences)\n","print(\"attention mask: {}\".format(attn_mask[0]))\n","\n","\n","#handle augmented data separately\n","if create_aug_data:\n","  print(\"preprocessing augmented data...\")\n","  a_sentences = [d[0] for d in aug_data]\n","  a_labels = [d[1] for d in aug_data]\n","  a_input_ids, a_attn_mask = tokenize_inputs_attn_mask(a_sentences)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["France is ‘ hunting down its citizens who joined twins ’ without trial in Iraq\n","0\n","Loading BERT tokenizer...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e19194e28a5d44e6889efb9e62a8abd5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"," Original:  France is ‘ hunting down its citizens who joined twins ’ without trial in Iraq\n","Tokenized:  ['france', 'is', '‘', 'hunting', 'down', 'its', 'citizens', 'who', 'joined', 'twins', '’', 'without', 'trial', 'in', 'iraq']\n","Token IDs:  [2605, 2003, 1520, 5933, 2091, 2049, 4480, 2040, 2587, 8178, 1521, 2302, 3979, 1999, 5712]\n","Original:  France is ‘ hunting down its citizens who joined twins ’ without trial in Iraq\n","Token IDs: [101, 2605, 2003, 1520, 5933, 2091, 2049, 4480, 2040, 2587, 8178, 1521, 2302, 3979, 1999, 5712, 102]\n","\n","Padding/truncating all sentences to 50 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","\n","Done.\n","[ 101 2605 2003 1520 5933 2091 2049 4480 2040 2587 8178 1521 2302 3979\n"," 1999 5712  102    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n","attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j6ZajGU7AWZh"},"source":["### split data into training and test datasets, convert to torch tensors...\n","import torch\n","import numpy as np\n","\n","#let's try test_pct = 0.1, 0.3, 0.5, 0.7, 0.8, 0.9\n","TEST_PCT = 0.1\n","outputs = tr_val_split(input_ids, labels, attn_mask, test_ratio = TEST_PCT)\n","train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = outputs\n","\n","#add augmented data to trdata ONLY\n","if create_aug_data:\n","  AUG_PCT = 0.05 #the rest will be converted to trdata...\n","  a_outputs = tr_val_split(a_input_ids, a_labels, a_attn_mask, test_ratio = AUG_PCT)\n","  a_train_inputs, _, a_train_labels, _, a_train_masks, _ = a_outputs\n","  train_inputs = torch.cat([train_inputs, a_train_inputs], dim=0)\n","  train_labels = torch.cat([train_labels, a_train_labels], dim=0)\n","  train_masks = torch.cat([train_masks, a_train_masks], dim=0)\n","  outputs = (train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks)\n","\n","#create train & validation iterators\n","batch_size=32\n","train_dataloader, validation_dataloader = create_data_iterators(*outputs, batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwHEn9f3wLZR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608283340056,"user_tz":480,"elapsed":333,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"d7bcfcaf-4e70-4d48-e998-50ae07819b78"},"source":["## print out the size of the training data...\n","print(\"training dataloader length: {}\".format(len(train_dataloader)))\n","print(\"validation dataloader length:   {}\".format(len(validation_dataloader)))\n","\n","## doublecheck\n","print(\"train inputs size: {}\".format(len(train_inputs)))\n","print(\"validation inputs size: {}\".format(len(validation_inputs)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training dataloader length: 504\n","validation dataloader length:   56\n","train inputs size: 16110\n","validation inputs size: 1790\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PyNIWwKaqNYx"},"source":["## Training\n","\n","Now, we actually start using BERT. We create the Bert model (from a pretrained model that we import) and apply it to the dataset."]},{"cell_type":"code","metadata":{"id":"tUlTx5y3m--9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608283343599,"user_tz":480,"elapsed":334,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"8244fd36-6ced-478b-e1e7-d3cdb0e67618"},"source":["## specify hardware.\n","import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-UY6qASIA1tQ","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b8ff4b1e309944c08a526cc4239af0a5","4de695b74d2c4a8096e06b9cf49f86da","8e0661b06ced4c35b40976b69957c59b","c5ec6b6b14b24a5da778d75c2d82dc1a","afe893dc0e904d00a97862e91e15f0a5","17f81827dfb14bc582537428ac70cde2","3edd8c4f97e34d7ea45ffe4410c619c7","3703904f2af64d6692d19ccc96e7c434","74a2fb1a96c3442198b276d84e83dd68","3eb9a84db6444e51ae13bb8c719764b6","c583f244b5644e91962b53454ff3405b","fa31687e1e4e43f7a33a380d736cd3b6","dddfba47771c4036b2a97942f083808e","4dbdd7f9dd2a43519f9b12af271a6afa","5e12839bf24b485a83375f67dce4cff6","041dd252ff414b8dbc9ffa67f85b4343"]},"executionInfo":{"status":"ok","timestamp":1608283373722,"user_tz":480,"elapsed":24014,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"f2ccba49-7863-45da-97a8-367d0e5a5658"},"source":["### RERUN THIS TOO: otherwise the model will remember the old trdata!\n","'''\n","  Create the BERT model.\n","  We will here import a pretrained bert model that has already been optimized for sequence classification.\n","'''\n","\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8ff4b1e309944c08a526cc4239af0a5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74a2fb1a96c3442198b276d84e83dd68","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"xk5A7Gv8menG"},"source":["#### RERUN (when changing trdata size)\n","from transformers import get_linear_schedule_with_warmup, AdamW\n","\n","## specify the optimizer\n","\n","# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","\n","## more model specs: specify the weight decay schedule.\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 2\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","\n","'''\n","  next step: specify some accuracy functions.\n","'''\n","import numpy as np\n","import time\n","import datetime\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAg2nEzUnJ_U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608283746140,"user_tz":480,"elapsed":307680,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"53fd3aa9-d8b9-47f1-d41b-87ee5336597a"},"source":["import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","#########\n","# DEBUG\n","#########\n","epochs = 2\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","# during validation, store output logits, labels & tokens so we can reconstruct errors.\n","out_logits = []\n","out_labels = []\n","in_tokens = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for step, batch in enumerate(validation_dataloader):\n","\n","        ### if even this doesn't work, I'm going to start calling a separte function in here to thoroughly debug.\n","        # Progress update every 40 batches (need to make sure the batches are correct...)\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)        \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(validation_dataloader), elapsed))\n","\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        #[DEBUG] what are the output shapes?\n","        #print(\"[debug] input_ids shape: \", b_input_ids.shape)\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        ### [DEBUG]\n","        #print(\"[debug] output logit shape:\", logits.shape)\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","        #save the inputs, logit results, and labels for analysis...\n","        if (epoch_i + 1) == epochs:\n","            out_logits.append(logits)\n","            out_labels.append(label_ids)\n","            in_tokens.append(b_input_ids)\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 2 ========\n","Training...\n","  Batch    40  of    504.    Elapsed: 0:00:11.\n","  Batch    80  of    504.    Elapsed: 0:00:22.\n","  Batch   120  of    504.    Elapsed: 0:00:34.\n","  Batch   160  of    504.    Elapsed: 0:00:45.\n","  Batch   200  of    504.    Elapsed: 0:00:56.\n","  Batch   240  of    504.    Elapsed: 0:01:08.\n","  Batch   280  of    504.    Elapsed: 0:01:19.\n","  Batch   320  of    504.    Elapsed: 0:01:31.\n","  Batch   360  of    504.    Elapsed: 0:01:43.\n","  Batch   400  of    504.    Elapsed: 0:01:54.\n","  Batch   440  of    504.    Elapsed: 0:02:06.\n","  Batch   480  of    504.    Elapsed: 0:02:19.\n","\n","  Average training loss: 0.65\n","  Training epcoh took: 0:02:26\n","\n","Running Validation...\n","  Accuracy: 0.66\n","  Validation took: 0:00:05\n","\n","======== Epoch 2 / 2 ========\n","Training...\n","  Batch    40  of    504.    Elapsed: 0:00:12.\n","  Batch    80  of    504.    Elapsed: 0:00:24.\n","  Batch   120  of    504.    Elapsed: 0:00:36.\n","  Batch   160  of    504.    Elapsed: 0:00:48.\n","  Batch   200  of    504.    Elapsed: 0:01:00.\n","  Batch   240  of    504.    Elapsed: 0:01:12.\n","  Batch   280  of    504.    Elapsed: 0:01:24.\n","  Batch   320  of    504.    Elapsed: 0:01:36.\n","  Batch   360  of    504.    Elapsed: 0:01:48.\n","  Batch   400  of    504.    Elapsed: 0:02:00.\n","  Batch   440  of    504.    Elapsed: 0:02:12.\n","  Batch   480  of    504.    Elapsed: 0:02:24.\n","\n","  Average training loss: 0.56\n","  Training epcoh took: 0:02:31\n","\n","Running Validation...\n","  Accuracy: 0.67\n","  Validation took: 0:00:05\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZU6yL9x_0sjw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608283775359,"user_tz":480,"elapsed":380,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"97577d87-06a5-4e67-e95b-156371cfa37b"},"source":["### here, let's take the validation data we saved and try to analyze it more.\n","preds = out_logits[0]\n","pred_flat = np.argmax(preds, axis=1).flatten()\n","print(preds)\n","print(pred_flat)\n","\n","# Q: how can we convert these into probabilities? Maybe divide by the max value out of (p_1 and p_0)?\n","#  >> the real answer: apply the logistic function to P(1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[-0.7716808   0.9887316 ]\n"," [ 0.01280305 -0.4718779 ]\n"," [-0.83237684  0.6538481 ]\n"," [-1.0247082   0.8997653 ]\n"," [-1.2322806   1.4947053 ]\n"," [-0.7150711   0.6086088 ]\n"," [-1.3890189   1.5529889 ]\n"," [ 0.3667687  -0.6053739 ]\n"," [-0.7295981   0.71150964]\n"," [-0.5343836   0.4676876 ]\n"," [-0.07085785 -0.21385741]\n"," [ 0.17543204 -0.35678414]\n"," [ 0.3066181  -0.68033576]\n"," [-0.16589732 -0.11649675]\n"," [-0.0424276  -0.2727683 ]\n"," [ 0.28689364 -0.3251935 ]\n"," [ 0.04426246 -0.2278276 ]\n"," [-0.73401254  0.44084877]\n"," [-0.29607287  0.14184856]\n"," [-0.5689494   0.24242856]\n"," [-1.1591725   1.2012486 ]\n"," [-1.176376    1.3645107 ]\n"," [-1.4069732   1.7613201 ]\n"," [-0.6248842   0.57534236]\n"," [ 0.21435316 -0.4745075 ]\n"," [ 0.178344   -0.403751  ]\n"," [-1.2056179   1.3040327 ]\n"," [ 0.07745255 -0.2429566 ]\n"," [-0.17310797 -0.04243121]\n"," [-1.0015625   0.9825573 ]\n"," [-0.19089189 -0.07894316]\n"," [-0.4505476   0.16781016]]\n","[1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mjV5TjNKMNMJ"},"source":["##Analysis of Generated Humor\n","\n","Using the BERT instance trained above, analyze the humor levels in the generated sentences."]},{"cell_type":"code","metadata":{"id":"4x0AedC5FwMQ"},"source":["'''\n","================ TODO NEXT ========================\n","\n","get results from our pipeline, package them up to be sent to BERT, and then analyze them.\n","  * Analyze the raw probabilities.\n","\n","~ might be nice to save this trained bert model? ## [Just forget this, it doesn't want to save the final classification layer :(]\n","\n","'''\n","\n","'''\n","#skip this cell, it's not working.\n","\n","# how to make this work: you have to use the same \"from_pretrained\" initializer as in the function above. (with n_classes parameter=2)\n","# problem: If you used that function, I don't think the classifier would be trained!\n","\n","# # step 1: save the pretrained model\n","# model.save_pretrained('gdrive/MyDrive/humor_generation/models/bert_humor_classifier')\n","\n","#step 2: extract pretrained model from directory. [haven't tested this yet]\n","from transformers import BertConfig, BertModel\n","\n","config1 = BertConfig.from_json_file('gdrive/MyDrive/humor_generation/models/bert_humor_classifier/config.json')\n","saved_model = BertModel.from_pretrained('gdrive/MyDrive/humor_generation/models/bert_humor_classifier/pytorch_model.bin', config= config1)\n","'''\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUQt_QGtPB0L"},"source":["# next: we need to tokenize, batch, create attention masks, and input into the model.\n","# more dependencies...\n","import sys\n","if 'gdrive/MyDrive/humor_generation/helpers' not in sys.path:\n","  sys.path.append('gdrive/MyDrive/humor_generation/helpers')\n","  print(sys.path)\n","  %run 'gdrive/MyDrive/humor_generation/helpers/prep_humor_data.py'\n","  %run 'gdrive/MyDrive/humor_generation/helpers/bert_fns.py'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfDAbeT1Qm89","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608184856097,"user_tz":480,"elapsed":994,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"51aeb593-c6b0-4be0-df87-f88baa68ce25"},"source":["# open the proper file and extract / preprocess the training data\n","import csv\n","fpath1 = 'gdrive/MyDrive/humor_generation/cbow_news/in_process_data/altered_preds_step1.csv'\n","\n","sent_ids = []\n","aug_output = []\n","with open(fpath1) as fpreds:\n","  jokereader = csv.reader(fpreds)\n","  next(jokereader)  # skip the header.\n","  for row in jokereader:\n","    sent = row[1]\n","    aug_output.append(sent)\n","    id = row[0]\n","    sent_ids.append(id)\n","  \n","print(aug_output[:5])\n","\n","# next, send this to the bert preprocessor.\n","# tokenize sentences and create attn masks\n","input_ids, attn_mask = tokenize_inputs_attn_mask(aug_output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['thousands of gay and bisexual men convicted of long abolished sexual senate are posthumously pardoned', 'thousands of gay and bisexual men convicted of long abolished sexual americans are posthumously pardoned', 'spanish police detain trump and search ripoll addresses in hunt for terror suspects', 'special prosecutor appointed to report', 'n y times reprimands trump for sharing unfounded rumor about melania trump']\n","Loading BERT tokenizer...\n"," Original:  thousands of gay and bisexual men convicted of long abolished sexual senate are posthumously pardoned\n","Tokenized:  ['thousands', 'of', 'gay', 'and', 'bisexual', 'men', 'convicted', 'of', 'long', 'abolished', 'sexual', 'senate', 'are', 'posthumously', 'pardon', '##ed']\n","Token IDs:  [5190, 1997, 5637, 1998, 22437, 2273, 7979, 1997, 2146, 8961, 4424, 4001, 2024, 12770, 14933, 2098]\n","Original:  thousands of gay and bisexual men convicted of long abolished sexual senate are posthumously pardoned\n","Token IDs: [101, 5190, 1997, 5637, 1998, 22437, 2273, 7979, 1997, 2146, 8961, 4424, 4001, 2024, 12770, 14933, 2098, 102]\n","\n","Padding/truncating all sentences to 50 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","\n","Done.\n","[  101  5190  1997  5637  1998 22437  2273  7979  1997  2146  8961  4424\n","  4001  2024 12770 14933  2098   102     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"TVrmrcYSSAmq","executionInfo":{"status":"ok","timestamp":1608184889119,"user_tz":480,"elapsed":339,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"0c329b04-944e-4edd-b240-4843c16bfeef"},"source":["# after the BERT tokenizer, create the tensor dataset & dataloader?\n","\n","import numpy as np\n","\n","# I think you can just convert the current data into numpy arrays, then into torch tensors.\n","validation_inputs = np.array(input_ids).astype(int)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_masks = torch.tensor(attn_mask)\n","\n","#let's try plugging this into the model.\n","\n","'''\n","outputs = model(b_input_ids, \n","                token_type_ids=None, \n","                attention_mask=b_input_mask)\n","'''\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\noutputs = model(b_input_ids, \\n                token_type_ids=None, \\n                attention_mask=b_input_mask)\\n'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":290},"id":"F9RJO8fPDutP","executionInfo":{"status":"ok","timestamp":1608110095845,"user_tz":480,"elapsed":818,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"4e8e011e-83d4-4ad0-9a09-638be12a1689"},"source":["# SKIP THIS CELL - it also eats up too much memory.\n","\n","#sample: just try to get this working on one example.\n","\n","print(len(aug_output))\n","print(validation_inputs.shape)\n","print(validation_masks.shape)\n","\n","#print out the first of each?\n","print(aug_output[0])\n","print(validation_inputs[0]) #lengths are slightly different, if we're really concerned we can investigate bert tokenizer.\n","\n","#run the model on just one...\n","print(\"\\nrunning bert...\")\n","op = model(validation_inputs[0].unsqueeze(0).to(device), token_type_ids=None, attention_mask=validation_masks[0].unsqueeze(0).to(device))\n","print(op[0].shape)\n","\n","print(\"\\n\")\n","print(op[0])\n","\n","'''\n","# outputs = model(b_input_ids, \n","#                token_type_ids=None, \n","#                attention_mask=b_input_mask)\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["34209\n","torch.Size([34209, 50])\n","torch.Size([34209, 50])\n","thousands of gay and bisexual men convicted of long abolished sexual senate are posthumously pardoned\n","tensor([  101,  5190,  1997,  5637,  1998, 22437,  2273,  7979,  1997,  2146,\n","         8961,  4424,  4001,  2024, 12770, 14933,  2098,   102,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n","\n","running bert...\n","torch.Size([1, 2])\n","\n","\n","tensor([[-0.7204,  0.1327]], device='cuda:0', grad_fn=<AddmmBackward>)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n# outputs = model(b_input_ids, \\n#                token_type_ids=None, \\n#                attention_mask=b_input_mask)\\n'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"BbZR3i8X10gv"},"source":["# create a tensor dataset and run the below.\n","\n","#requires the function, \"create_data_iterators\". This is a very simple function, do it tmrw.\n","\n","def create_data_itr(validation_inputs, validation_masks, batch_size=32):\n","  validation_data = TensorDataset(validation_inputs, validation_masks)\n","  validation_sampler = SequentialSampler(validation_data)\n","  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","  return validation_dataloader\n","    \n","\n","batch_size=32\n","test_dataloader = create_data_itr(validation_inputs, validation_masks, batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8oNIQizrJQs","executionInfo":{"status":"ok","timestamp":1608184909343,"user_tz":480,"elapsed":2157,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"ca9a7f64-5fb6-4abe-b378-2575e580cfe2"},"source":["# generate scores for validation.\n","\n","out_logits = []\n","in_tokens = []\n","\n","for step, batch in enumerate(test_dataloader):\n","\n","        # Progress update every 40 batches (need to make sure the batches are correct...)\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)        \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask = batch\n","\n","        #[DEBUG] what are the output shapes?\n","        # print(\"[debug] input_ids shape: \", b_input_ids.shape)\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        ### [DEBUG]\n","        # print(\"[debug] output logit shape:\", logits.shape)\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        \n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","        #save the inputs, logit results, and labels for analysis...\n","        if (epoch_i + 1) == epochs:\n","            out_logits.append(logits)\n","            in_tokens.append(b_input_ids)\n","\n","    # Report the final accuracy for this validation run.\n","    #print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\\n output logits:\")\n","print(out_logits[:10])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," output logits:\n","[array([[-0.10259536, -0.0302136 ],\n","       [ 0.26917467, -0.3265173 ],\n","       [-0.45545426,  0.8180601 ],\n","       [ 0.5370537 , -0.24718463],\n","       [-0.5410654 ,  0.6365072 ],\n","       [-0.16520555,  0.51062095],\n","       [ 0.24478681,  0.20929036],\n","       [ 0.5888642 , -0.29840088],\n","       [ 0.38325027, -0.01231874],\n","       [ 0.784606  , -0.47141433],\n","       [ 0.7479833 , -0.4496566 ],\n","       [ 0.5498434 , -0.29440725],\n","       [ 0.5148533 , -0.26997465],\n","       [ 0.5207918 , -0.22197433],\n","       [-0.06434891,  0.20570123],\n","       [-0.18165816,  0.3894839 ],\n","       [ 0.00464029,  0.44292805],\n","       [ 0.31987017,  0.08765332],\n","       [ 0.70630366, -0.5098017 ],\n","       [ 0.32669526, -0.2177981 ],\n","       [-0.0561959 ,  0.08650753],\n","       [ 0.3375068 , -0.21853232],\n","       [-0.01580168,  0.16879542],\n","       [ 0.34226826, -0.16985291],\n","       [ 0.3460017 , -0.19717465],\n","       [ 0.4170067 , -0.2123852 ],\n","       [ 0.18163233, -0.03807513],\n","       [ 0.21730761,  0.06771749],\n","       [ 0.51008195, -0.40769422],\n","       [-0.04377226,  0.05663009],\n","       [ 0.32767957,  0.30619285],\n","       [-0.88509554,  1.3168144 ]], dtype=float32), array([[ 0.26779017,  0.32813308],\n","       [ 0.7721124 , -0.2712176 ],\n","       [-0.42155182,  0.6935973 ],\n","       [ 0.7452427 , -0.19846418],\n","       [ 0.72782224, -0.47419477],\n","       [ 0.6970991 , -0.47844267],\n","       [-0.2295049 ,  0.27191332],\n","       [-0.71577877,  1.033956  ],\n","       [ 0.38869974, -0.20326503],\n","       [-0.48861828,  0.7210898 ],\n","       [-0.15087736,  0.44839108],\n","       [ 0.42398152, -0.14667523],\n","       [ 0.434193  , -0.12369479],\n","       [ 0.4123179 , -0.21720618],\n","       [ 0.23161331,  0.07208362],\n","       [-0.2135487 ,  0.67356026],\n","       [ 1.0170944 , -0.8432812 ],\n","       [ 0.90759   , -0.6979398 ],\n","       [-0.38516408,  0.7283004 ],\n","       [ 0.34860602, -0.02816938],\n","       [ 0.83832526, -0.576169  ],\n","       [ 0.30927646, -0.08045119],\n","       [-0.13102235,  0.4694949 ],\n","       [ 0.00285125,  0.22467229],\n","       [-0.2589992 ,  0.5248998 ],\n","       [ 0.29666245, -0.00890708],\n","       [ 0.13010415,  0.28689703],\n","       [ 0.74442863, -0.45662042],\n","       [ 0.636592  , -0.29751128],\n","       [ 0.49803683, -0.04497179],\n","       [ 0.3632083 ,  0.11343381],\n","       [ 0.282146  ,  0.40948117]], dtype=float32), array([[ 0.32258067,  0.3638229 ],\n","       [ 0.29147184,  0.41877905],\n","       [ 0.7386629 , -0.33270776],\n","       [ 0.27170107, -0.00654145],\n","       [ 0.33512402,  0.25534883],\n","       [ 0.723027  , -0.623482  ],\n","       [ 0.59402984, -0.21270764],\n","       [ 0.8729922 , -0.67797655],\n","       [ 0.6367956 , -0.3593729 ],\n","       [ 0.43345773, -0.14620578],\n","       [ 0.07071286,  0.29884276],\n","       [ 0.4252302 , -0.1402094 ],\n","       [ 0.7974156 , -0.44631502],\n","       [-0.21222812,  0.50127447],\n","       [ 0.624054  , -0.3126338 ],\n","       [ 0.6149988 , -0.35049233],\n","       [ 0.5815443 , -0.09406649],\n","       [ 0.1285153 ,  0.21153605],\n","       [ 0.62086225, -0.33475265],\n","       [ 0.97697246, -0.5432894 ],\n","       [ 0.68678737, -0.31839278],\n","       [ 0.80519724, -0.38495186],\n","       [ 0.82443154, -0.42413858],\n","       [ 0.67896867, -0.49803385],\n","       [ 0.60460013, -0.48820132],\n","       [ 0.5745755 , -0.44565946],\n","       [ 0.18974832, -0.19891334],\n","       [ 0.449467  , -0.47166955],\n","       [ 0.49484417, -0.07142653],\n","       [ 0.5088971 , -0.09157319],\n","       [-0.13961442,  0.5168635 ],\n","       [ 0.10500504,  0.04732531]], dtype=float32), array([[-0.42228618,  1.0121281 ],\n","       [-0.33154964,  0.5059584 ],\n","       [-0.14534338,  0.28775385],\n","       [-0.39947665,  1.0413873 ],\n","       [ 0.28072026,  0.27439746],\n","       [-0.11230208,  0.59687936],\n","       [ 0.8766439 , -0.60082096],\n","       [ 0.8298319 , -0.40974507],\n","       [ 0.6687794 , -0.24138872],\n","       [ 0.91414744, -0.5162688 ],\n","       [ 0.79099363, -0.39692017],\n","       [ 0.8840145 , -0.44307435],\n","       [-0.49469718,  0.46278262],\n","       [ 0.7979497 , -0.47266605],\n","       [-0.5676088 ,  0.6712296 ],\n","       [-0.70673573,  0.72161716],\n","       [-1.0065904 ,  1.192259  ],\n","       [-0.4941112 ,  0.6568606 ],\n","       [-1.0852013 ,  1.3788117 ],\n","       [-1.142033  ,  1.4491961 ],\n","       [-0.05834776,  0.2823381 ],\n","       [ 0.6768628 , -0.20557852],\n","       [-0.26610503,  0.40593067],\n","       [ 0.08403965,  0.22058967],\n","       [-0.18797216,  0.6302182 ],\n","       [ 0.08297893,  0.19441515],\n","       [ 0.6675628 , -0.46611258],\n","       [ 0.50156426, -0.32236102],\n","       [ 0.5439622 , -0.29065382],\n","       [-0.6602496 ,  0.8423411 ],\n","       [-0.6643563 ,  0.7992234 ],\n","       [ 0.22903019, -0.12445498]], dtype=float32), array([[ 0.577299  , -0.6076769 ],\n","       [ 0.5876067 , -0.26073375],\n","       [ 0.42523542, -0.04837202],\n","       [ 0.24416658, -0.18080336],\n","       [ 0.42380378, -0.00887236],\n","       [ 0.38017502, -0.03572103],\n","       [ 0.5715458 , -0.2660409 ],\n","       [ 0.7695885 , -0.4487919 ],\n","       [ 0.39606595, -0.22941749],\n","       [ 0.74678355, -0.40754396],\n","       [ 0.55527174, -0.22603461],\n","       [-0.15963237,  0.35838917],\n","       [-0.0561959 ,  0.08650753],\n","       [-0.32818905,  0.44128016],\n","       [-0.5979957 ,  0.9005653 ],\n","       [ 0.55495614, -0.35431883],\n","       [-0.18145637,  0.54790735],\n","       [ 0.47192228, -0.2749139 ],\n","       [ 0.9032872 , -0.48950914],\n","       [ 0.22903019, -0.12445498],\n","       [ 0.933377  , -0.5607485 ],\n","       [ 0.12939408,  0.10094886],\n","       [-0.5403401 ,  0.7578961 ],\n","       [ 0.3579923 , -0.02177128],\n","       [ 0.7639997 , -0.50103515],\n","       [ 0.7147001 , -0.4362258 ],\n","       [ 0.86907864, -0.5641207 ],\n","       [ 0.9000222 , -0.40708995],\n","       [ 0.799929  , -0.45674238],\n","       [ 0.8334286 , -0.63459426],\n","       [ 0.81718355, -0.42767745],\n","       [ 0.6046948 , -0.2840751 ]], dtype=float32), array([[ 0.456367  , -0.10931654],\n","       [ 0.8150026 , -0.48975602],\n","       [ 0.56316406, -0.24874413],\n","       [ 0.5998203 , -0.2270159 ],\n","       [ 0.7480616 , -0.32677722],\n","       [ 0.48120746,  0.09382617],\n","       [ 0.05016361,  0.31189558],\n","       [-1.1132039 ,  1.2068086 ],\n","       [ 0.44894525, -0.17789707],\n","       [-0.26902258,  0.44917598],\n","       [ 0.46714297, -0.14600898],\n","       [ 0.67447466, -0.2831229 ],\n","       [ 0.7630201 , -0.3817713 ],\n","       [ 0.4476176 , -0.13767396],\n","       [-0.30883417,  0.74377304],\n","       [-0.38314214,  0.70486104],\n","       [ 0.5878986 , -0.15428106],\n","       [-0.5462718 ,  1.0165541 ],\n","       [ 0.5963355 , -0.09762385],\n","       [-0.81058276,  1.1196067 ],\n","       [ 0.5427108 , -0.11176381],\n","       [-0.73734623,  1.0765444 ],\n","       [-0.7056231 ,  0.5411906 ],\n","       [-0.5782351 ,  0.97051257],\n","       [ 0.29068324, -0.16727242],\n","       [ 0.1203286 ,  0.03569176],\n","       [ 0.2831757 , -0.18172164],\n","       [ 0.6393712 , -0.2659456 ],\n","       [ 0.61611986, -0.05166259],\n","       [ 0.7738175 , -0.5904294 ],\n","       [ 0.5839771 , -0.15261802],\n","       [-0.74592495,  1.1430331 ]], dtype=float32), array([[-0.55806524,  1.0026754 ],\n","       [ 0.05107621,  0.11887704],\n","       [-0.388107  ,  0.7473975 ],\n","       [ 0.9937367 , -0.67177844],\n","       [ 0.36565793, -0.09495695],\n","       [ 0.8796347 , -0.65803313],\n","       [ 0.7721124 , -0.2712176 ],\n","       [ 0.22947468, -0.02464178],\n","       [ 0.7452427 , -0.19846418],\n","       [ 0.6109164 , -0.11646122],\n","       [ 0.19527242, -0.17603892],\n","       [ 0.4996979 , -0.18965673],\n","       [ 0.9825338 , -0.666082  ],\n","       [ 0.14787288,  0.18624488],\n","       [ 1.0809293 , -0.6932023 ],\n","       [ 0.20408136,  0.13053524],\n","       [ 0.79383975, -0.70171845],\n","       [ 0.7873355 , -0.7396099 ],\n","       [ 0.73059016, -0.595059  ],\n","       [ 0.21922457,  0.21780728],\n","       [ 0.46719167,  0.00802778],\n","       [ 0.56316406, -0.24874413],\n","       [ 0.5998203 , -0.2270159 ],\n","       [ 0.24683675, -0.05213577],\n","       [ 0.5023451 , -0.2656912 ],\n","       [ 0.04660806,  0.3007681 ],\n","       [ 0.02026106, -0.06103898],\n","       [ 0.34860602, -0.02816938],\n","       [-0.42655364,  0.72583675],\n","       [ 0.06849602,  0.24209295],\n","       [ 0.48990807, -0.04871938],\n","       [ 0.31965145,  0.04953669]], dtype=float32), array([[ 0.70251155, -0.13091187],\n","       [ 0.6104676 , -0.10093726],\n","       [-0.02017127,  0.42814294],\n","       [ 0.9578037 , -0.71262586],\n","       [ 0.7913086 , -0.48875004],\n","       [ 1.0887592 , -0.767956  ],\n","       [-0.20460355,  0.5613522 ],\n","       [ 0.34198564,  0.11902116],\n","       [-0.08412904,  0.27800494],\n","       [ 0.7869216 , -0.28857958],\n","       [ 0.24727917,  0.02336319],\n","       [ 0.6618933 , -0.27056408],\n","       [ 0.12909618,  0.28577968],\n","       [ 0.3359393 , -0.38470703],\n","       [ 0.76043725, -0.5698628 ],\n","       [ 0.32854453,  0.04263581],\n","       [ 0.6320593 , -0.3573068 ],\n","       [-0.6144227 ,  0.69220716],\n","       [-0.29862598,  0.6869884 ],\n","       [-0.20918503,  0.6072397 ],\n","       [ 0.3335351 , -0.13394083],\n","       [-1.2415043 ,  1.7421658 ],\n","       [ 0.7869216 , -0.28857958],\n","       [ 0.24727917,  0.02336319],\n","       [ 0.6618933 , -0.27056408],\n","       [ 0.54170644, -0.52161455],\n","       [ 0.9181641 , -0.68769914],\n","       [ 0.8288272 , -0.6816022 ],\n","       [ 0.825473  , -0.70557433],\n","       [-0.09021936,  0.2715259 ],\n","       [ 0.79945356, -0.6482911 ],\n","       [ 0.46247002, -0.12484513]], dtype=float32), array([[ 0.60703087, -0.3441239 ],\n","       [ 0.48990807, -0.04871938],\n","       [ 0.31965145,  0.04953669],\n","       [ 0.26143065, -0.11566392],\n","       [ 0.3292919 , -0.17609233],\n","       [-0.01924429,  0.04387201],\n","       [ 0.4685772 , -0.2394388 ],\n","       [ 1.0174063 , -0.67873067],\n","       [ 0.87238234, -0.50532776],\n","       [ 0.8269414 , -0.53871614],\n","       [ 0.33001176, -0.03503421],\n","       [ 0.68645203, -0.3115346 ],\n","       [ 0.6317989 , -0.26047605],\n","       [ 0.8288803 , -0.31442618],\n","       [ 0.7967572 , -0.2577231 ],\n","       [ 0.59217113, -0.2367744 ],\n","       [ 0.5806667 , -0.3796138 ],\n","       [ 0.47860718, -0.09704949],\n","       [ 0.8148979 , -0.6519355 ],\n","       [ 0.71301407, -0.64897394],\n","       [ 0.7365413 , -0.4623449 ],\n","       [ 0.85416347, -0.60588276],\n","       [ 0.8837179 , -0.7457395 ],\n","       [ 0.92236435, -0.66583097],\n","       [-0.21108384,  0.6861823 ],\n","       [-0.10402502,  0.56667477],\n","       [ 0.01153595,  0.3902023 ],\n","       [ 0.06674436,  0.37080935],\n","       [ 0.15952703,  0.27148324],\n","       [ 0.16940832, -0.1832032 ],\n","       [ 0.37223807, -0.40079293],\n","       [ 0.26917467, -0.3265173 ]], dtype=float32), array([[ 0.7811304 , -0.5718533 ],\n","       [ 0.4252302 , -0.1402094 ],\n","       [ 0.41033152,  0.03154602],\n","       [ 0.6977835 , -0.17999175],\n","       [ 0.2161319 ,  0.25112805],\n","       [ 0.45069155, -0.3370883 ],\n","       [ 0.37340507, -0.17274183],\n","       [-0.09021936,  0.2715259 ],\n","       [-0.2099841 ,  0.43421653],\n","       [ 0.7841163 , -0.25314963],\n","       [ 0.8510833 , -0.35279444],\n","       [ 0.27482757,  0.04533869],\n","       [-0.55806524,  1.0026754 ],\n","       [-0.24293979,  0.4303811 ],\n","       [-0.388107  ,  0.7473975 ],\n","       [ 0.65398663, -0.38269517],\n","       [ 0.21895325,  0.03758899],\n","       [ 0.6137483 , -0.4178259 ],\n","       [ 0.5317083 , -0.16383249],\n","       [ 0.63623655, -0.45494035],\n","       [ 0.71372986, -0.35773548],\n","       [ 1.0488039 , -0.7343553 ],\n","       [-0.24080312,  0.41888148],\n","       [-0.12190963,  0.19590186],\n","       [-0.42657873,  0.63623136],\n","       [ 0.0653317 ,  0.3318153 ],\n","       [-0.4092731 ,  0.5473198 ],\n","       [-0.16388568,  0.5779384 ],\n","       [ 0.2732487 ,  0.03057982],\n","       [-0.03849947,  0.3631986 ],\n","       [ 0.11595423,  0.15557808],\n","       [ 0.31596446,  0.01479589]], dtype=float32)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ziAGL9t6yHFu"},"source":["#next step: save the evaluation results\n","\n","#before we save the output, we want to combine all the arrays into a single \"1070 x 2\" array.\n","out_logits_ccat = np.vstack(out_logits)\n","print(out_logits_ccat.shape)\n","\n","#print out the input sentence, as well as the two entries of the logit array.\n","import csv\n","\n","fpath = 'gdrive/MyDrive/humor_generation/cbow_news/in_process_data/evaluation_2.csv'\n","with open(fpath, 'w') as fout:\n","  jokewriter = csv.writer(fout)\n","  jokewriter.writerow([\"id\", \"sentence\", \"not_funny_logit\", \"funny_logit\"])\n","  for i in range(len(aug_output)):\n","    id = sent_ids[i]\n","    s = aug_output[i]\n","    l = out_logits_ccat[i]\n","    row = [id, s, l[0], l[1]]\n","    print(row)\n","    jokewriter.writerow(row)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"id":"x1GYqFShGjXc","executionInfo":{"status":"ok","timestamp":1608284104478,"user_tz":480,"elapsed":537,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"d55f0054-21ea-409c-bf4b-374043704e5f"},"source":["# we also want to analyze the results.\n","\n","import matplotlib.pyplot as plt\n","from scipy.special import expit\n","\n","\n","\n","#first, apply a sigmoid to the \"funny\" score.\n","# x = expit(out_logits_ccat[:1000, 1])\n","\n","\n","#out_logits_ccat = np.vstack(out_logits)\n","#print(out_logits_ccat.shape)\n","x = expit(out_logits_ccat[:1000, 1])\n","\n","plt.hist(x=x)\n","plt.title(\"Output humor scores from BERT (Human Data)\")\n","\n","# then, create a histogram showing frequency of funny and non-funny results.\n","\n","\n","# this would be good to compare with a histogram of the funny results in the other dataest."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Output humor scores from BERT (Human Data)')"]},"metadata":{"tags":[]},"execution_count":17},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYOklEQVR4nO3de5xcZX3H8c9XwkUQCJAlQhLYKHgJoNWuoLWtKNZyE2hrYyhioNSUFrStKJdWC2hRoFaqorZRkKgIRLQSvFMEqRZoF6RcgkgIQRIuWSAhgMrNX/94npXDMJudmTO7s3n2+3699pWZc3t+58zsd57znNkTRQRmZlaW5/W6ADMz6z6Hu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuGwBJe0ta2es6SiPp+ZIulfSwpK/2up5ektQn6aeSnt/rWjZEkt4q6aJe11E1KcNd0hGSbpL0C0n3SfqspKltrL9C0pu7WE9Xt2ctexswHdguIv50vBuXdKWkX0l6NH/AXCVpj8r8UyQ9mecP/6ytzA9Jj+XpqyR9XNJGkr5TWf5JSU9Unv/bCOWcCJwXEb+s1PYXDfVuEJ2M/Pv9dGWf75T0BUkvaWMb50n6p1aXj4hLgd0kvaKjosfApAt3SccBZwDvB7YGXgvsDFwmaZNe1rahkTRlA29zZ+BnEfHUOLQ1kmMj4gXAtsCVwJca5l8UES+o/DR2Ql6Z138D8HbgzyNiv+HlgfOBMyvrH91YgKRNgfnAl7u8b710dd7/rYE3A78ErpO0+xi2eQGwYAy335ZJFe6StgJOBd4dEd+NiCcjYgUwF+gH3pGXe9andrXHIulLwE7ApblXcLyk/tyLWiDpHkn3SnpfZf22tree+o+TtDpv/8jK9Gf1snLP5UeV5yHpryXdLukRSR+W9GJJ/y1pnaTF1Q82Se+StEzSQ5KWSNqxYVvHSLoduL1JjZtJ+rKkByWtlfS/kqbnedvmHtQ9ktZI+kanbUo6UNINuY3/rvaYJJ2Qe7KPSLpN0j5N6jwV+Efg7fm4H5WP248lnSXpQeAUSVtL+qKkIUl3SfqApOdVjvPw8mslLZf0O3n63fm1mj/S61kVEU8DFwJzWlm+yfrLgB8Dv9XB6nsBayOirV65Gs44lc40vpwfD/9OHJmPxRpJR0t6jaQb8/E6u7LuiyX9IL9vHpB0vipn07mt9+V1H5Z0kaTNRqsxIp6OiDsi4q+BHwKnVLb5VaUz9+Gzpt3y9AXAYcDx+b1xaZ5+oqQ78vtqqaQ/amjuSuCAdo7hWJpU4Q78DrAZ8PXqxIh4FPg28AejbSAiDgd+Drw194TOrMx+I7Ar8BbgBLUw1DLK9qpeSOqFzACOAj4taZvRtl/xh8Bvk85UjgcWkj7MZgG7A4cCSHoT8FHSB94OwF2k0Kk6hBQIzYJofq5zFrAdcDSp1wSpV7o5sBuwPXBWJ21KehVwLvCXuY1/B5ZI2lTSS4FjgddExJZ5v1c0FhkRJwMf4Zme8Tl51l7ActJwzWnAp/L+vIjUO34ncGRlU3sBN+Y6vpLrfg2wC+n4ni3pBU2O07PkD9fDgGtGW3aE9V8G/B6wrIPV9wBu66TdFuxF+p14O/CvwD+QetK7AXMlvSEvJ9J7YEfg5aT3zykN25oL7AvMBl4BHNFmLV8nHaNh38m1bQ9cTzrLISIW8uwznrfm5e/I629N6iR+WdIOle3dCvQrdSJ7brKF+zTggRFOw+/N8+s4NSIei4ibgC+QA7NLngQ+lM82vg08Cry0jfXPjIh1EXELcDPw/YhYHhEPk97kr8rLHQacGxHXR8TjwEnA6yT1V7b10Yh4aHh8tkmd2wG75F7TdRGxLv8S7AccHRFr8n78sMM2FwD/HhHX5jYWAY+TPrieBjYlfQhsHBErIuKONo7TPRHxqfweeQKYB5wUEY/ks7x/AQ6vLH9nRHwh97wvIoXShyLi8Yj4ft7GLutp75NK4+iPkD6UTm2YPzf3cod/rmiYf72kx0jBciXwmTb2ddjU3H7T2oZ/gG92sO0PR8Sv8rF4DLggIlZHxCrgv8jvu4hYFhGX5eM2BHyc9GH6rHoi4p6IeAi4lPbPUu4hDX+R2zw3v66Pkz5IXilp65FWjoiv5vZ/HREXkc4i96wsMnwMW75+N5YmW7g/AExT87HUHfL8Ou6uPL6L1AvplgcbPpR+AYzaI6y4v/L4l02eD29rR1LtwG/Oah4knTEMq+5noy8B3wMuzMMvZ0ramBR6D0XEmibrtNvmzsBxDcEzC9gxD0/8LemXdbWkC6tDPC2otjMN2LhaW35cravxOBIRIx3bZt6Tx9GfDxwIXKxnX5RbHBFTKz9vbFj/1Xn7byf1krdYT1sjWQNsOVJtwz+5vna19L6TND2/VqskrSON/zd2tu6rPG73/Q/pdXsot7eRpNPzMMs6njm7G7GDJ+mdlaHAtaQz3uryw8dw7XPXHn+TLdyvJvXw/rg6MZ827wdcnic9Rho+GPbChu2MdCvNWZXHO5F6CnW216rRtt+Oe0jhCYCkLUg98VWVZUasN/fIT42IOaRhsANJQxl3A9uq+beS2m3zbuC0htDbPCIuyDV8JSJ+N28zSBfQW1Vt5wHSmcjOlWk7NdTVFbk3+F+kYZW3tLluRMRi0vv7Hzto/kag5W+SVHTzffcR0rHfIyK2Ig1pqcb2mvkj0tkCwJ8BB5OGiLYmXXOj0uaz3uOSdgY+Rzq72i5/2N3cUOPLgRURsa7LdXdkUoV7HoI4FfiUpH0lbZxP/RcDK3nmmwo3APsrXQB8IaknWHU/aQy20QclbZ4vzBxJOk2vs71W3QD8cW57F9KYfKcuAI6U9FtK36L4CHBtHpIYlaQ3StpD0kbAOlI4/joi7iUN/3xG0jb52P9+h21+Djha0l5KtpB0gKQtJb1U0pvydn5F6h3+upMDkYdaFgOn5W3vDLyXMfpWiaTXka5j3NLhJk4H3pXfY+34H2CqpBmjLvlsNwDz8ms5QPpqaae2JA01PpzreH+Nbf1G7qHPlvQpYG+eGfbaktTRe5D0AfWRhlUbfye3IAX+UN7ukaSee9UbSO/xCWFShTtAvmD598DHSOFzLaknuE8ee4MU8v9HOlX7Ps+E9LCPAh/Ip2fvq0z/IanndTnwsTzOWGd7rTqLNLZ7P7CIfGGoExHxn8AHga+RrkO8mDTu3KoXAheTju2tpGMy/KF5OCnsfwqsJn/ItdtmRAwC7wLOJg0pLOOZi2ubkkLuAdJp/PakMfxOvZvUQ10O/Ih00fTcGttrdHb+RsajpOP0gYioBsTwt3mqP9s321C+1nMVbQZjRDwBnEf+tlgbPkh6rdaQQvMrba5fdSppiOlh4Fs0fOmhA6/Lx3Qd6VrEVqSL7Dfl+V8kDbGtApby3AvZ55Cu26yV9I2IWEq63nI16fdsD9K3k6oOJV3cnxAU/s86asu9/zuBjUe4WGs2oUnqI1/gHOFCua2HpLcCh0fE3F7XMszh3gUOdzObaCbdsIyZ2WTgnruZWYHcczczK9C43/ipmWnTpkV/f3+vyzAz26Bcd911D0REX7N5EyLc+/v7GRwc7HUZZmYbFEl3jTTPwzJmZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgWaEH+hau3pP/FbPWt7xekH9KxtM2ude+5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFci3HzAbRa9u99CrWz349hZlcM/dzKxADnczswI53M3MCjRquEs6V9JqSTdXpv2zpJ9KulHSf0iaWpl3kqRlkm6T9IdjVbiZmY2slZ77ecC+DdMuA3aPiFcAPwNOApA0B5gH7JbX+YykjbpWrZmZtWTUcI+Iq4CHGqZ9PyKeyk+vAWbmxwcDF0bE4xFxJ7AM2LOL9ZqZWQu6Meb+58B38uMZwN2VeSvzNDMzG0e1vucu6R+Ap4DzO1h3AbAAYKeddqpTho2jyfadb7MNVcc9d0lHAAcCh0VE5MmrgFmVxWbmac8REQsjYiAiBvr6+jotw8zMmugo3CXtCxwPHBQRv6jMWgLMk7SppNnArsD/1C/TzMzaMeqwjKQLgL2BaZJWAieTvh2zKXCZJIBrIuLoiLhF0mJgKWm45piIeHqsijczs+ZGDfeIOLTJ5HPWs/xpwGl1ijIzs3r8F6pmZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFajWLX/NxkuvbjVstqFyz93MrEAOdzOzAnlYxswmDP9PX93jnruZWYEc7mZmBXK4m5kVyGPuZhOUv/5pdbjnbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWoFHDXdK5klZLurkybVtJl0m6Pf+7TZ4uSZ+UtEzSjZJePZbFm5lZc6303M8D9m2YdiJweUTsClyenwPsB+yafxYAn+1OmWZm1o5Rwz0irgIeaph8MLAoP14EHFKZ/sVIrgGmStqhW8WamVlrOh1znx4R9+bH9wHT8+MZwN2V5Vbmac8haYGkQUmDQ0NDHZZhZmbN1L6gGhEBRAfrLYyIgYgY6Ovrq1uGmZlVdBru9w8Pt+R/V+fpq4BZleVm5mlmZjaOOr23zBJgPnB6/veSyvRjJV0I7AU8XBm+KY7v/WFmE9Wo4S7pAmBvYJqklcDJpFBfLOko4C5gbl7828D+wDLgF8CRY1CzmZmNYtRwj4hDR5i1T5NlAzimblFmZlaP/0LVzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswLVCndJfyfpFkk3S7pA0maSZku6VtIySRdJ2qRbxZqZWWs6DndJM4D3AAMRsTuwETAPOAM4KyJ2AdYAR3WjUDMza13dYZkpwPMlTQE2B+4F3gRcnOcvAg6p2YaZmbWp43CPiFXAx4Cfk0L9YeA6YG1EPJUXWwnMaLa+pAWSBiUNDg0NdVqGmZk1UWdYZhvgYGA2sCOwBbBvq+tHxMKIGIiIgb6+vk7LMDOzJuoMy7wZuDMihiLiSeDrwOuBqXmYBmAmsKpmjWZm1qY64f5z4LWSNpckYB9gKXAF8La8zHzgknolmplZu+qMuV9LunB6PXBT3tZC4ATgvZKWAdsB53ShTjMza8OU0RcZWUScDJzcMHk5sGed7ZqZWT3+C1UzswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrUK1wlzRV0sWSfirpVkmvk7StpMsk3Z7/3aZbxZqZWWvq9tw/AXw3Il4GvBK4FTgRuDwidgUuz8/NzGwcdRzukrYGfh84ByAinoiItcDBwKK82CLgkLpFmplZe+r03GcDQ8AXJP1E0uclbQFMj4h78zL3AdObrSxpgaRBSYNDQ0M1yjAzs0Z1wn0K8GrgsxHxKuAxGoZgIiKAaLZyRCyMiIGIGOjr66tRhpmZNaoT7iuBlRFxbX5+MSns75e0A0D+d3W9Es3MrF0dh3tE3AfcLemledI+wFJgCTA/T5sPXFKrQjMza9uUmuu/Gzhf0ibAcuBI0gfGYklHAXcBc2u2YWZmbaoV7hFxAzDQZNY+dbZrZmb1+C9UzcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAtX6D7LNzErQf+K3etb2itMPGJPtuuduZlag2uEuaSNJP5H0zfx8tqRrJS2TdJGkTeqXaWZm7ehGz/1vgFsrz88AzoqIXYA1wFFdaMPMzNpQK9wlzQQOAD6fnwt4E3BxXmQRcEidNszMrH11e+7/ChwP/Do/3w5YGxFP5ecrgRnNVpS0QNKgpMGhoaGaZZiZWVXH4S7pQGB1RFzXyfoRsTAiBiJioK+vr9MyzMysiTpfhXw9cJCk/YHNgK2ATwBTJU3JvfeZwKr6ZZqZWTs67rlHxEkRMTMi+oF5wA8i4jDgCuBtebH5wCW1qzQzs7aMxffcTwDeK2kZaQz+nDFow8zM1qMrf6EaEVcCV+bHy4E9u7FdMzPrjP9C1cysQA53M7MCOdzNzArkcDczK5DD3cysQBv8/dx7eR9mM7OJyj13M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQB2Hu6RZkq6QtFTSLZL+Jk/fVtJlkm7P/27TvXLNzKwVdXruTwHHRcQc4LXAMZLmACcCl0fErsDl+bmZmY2jjsM9Iu6NiOvz40eAW4EZwMHAorzYIuCQukWamVl7ujLmLqkfeBVwLTA9Iu7Ns+4Dpo+wzgJJg5IGh4aGulGGmZlltcNd0guArwF/GxHrqvMiIoBotl5ELIyIgYgY6Ovrq1uGmZlV1Ap3SRuTgv38iPh6nny/pB3y/B2A1fVKNDOzdtX5toyAc4BbI+LjlVlLgPn58Xzgks7LMzOzTkypse7rgcOBmyTdkKf9PXA6sFjSUcBdwNx6JZqZWbs6DveI+BGgEWbv0+l2zcysPv+FqplZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRVozMJd0r6SbpO0TNKJY9WOmZk915iEu6SNgE8D+wFzgEMlzRmLtszM7LnGque+J7AsIpZHxBPAhcDBY9SWmZk1mDJG250B3F15vhLYq7qApAXAgvz0UUm3jVEt6zMNeKAH7Y6VkvanpH0B789E1tN90Rm1Vt95pBljFe6jioiFwMJetQ8gaTAiBnpZQzeVtD8l7Qt4fyaykvalaqyGZVYBsyrPZ+ZpZmY2DsYq3P8X2FXSbEmbAPOAJWPUlpmZNRiTYZmIeErSscD3gI2AcyPilrFoq6aeDguNgZL2p6R9Ae/PRFbSvvyGIqLXNZiZWZf5L1TNzArkcDczK9CkCPfRboUg6WhJN0m6QdKPJvJf07Z6WwdJfyIpJE3or3i18NocIWkovzY3SPqLXtTZqlZeH0lzJS2VdIukr4x3ja1q4bU5q/K6/EzS2l7U2aoW9mcnSVdI+omkGyXt34s6uyYiiv4hXdC9A3gRsAnwf8CchmW2qjw+CPhur+vudF/yclsCVwHXAAO9rrvma3MEcHava+3i/uwK/ATYJj/fvtd113mvVZZ/N+mLEz2vvcZrsxD4q/x4DrCi13XX+ZkMPfdRb4UQEesqT7cAJupV5lZv6/Bh4AzgV+NZXAdKu01FK/vzLuDTEbEGICJWj3ONrWr3tTkUuGBcKutMK/sTwFb58dbAPeNYX9dNhnBvdiuEGY0LSTpG0h3AmcB7xqm2do26L5JeDcyKiG+NZ2Edaum1Af4knyZfLGlWk/kTRSv78xLgJZJ+LOkaSfuOW3XtafW1QdLOwGzgB+NQV6da2Z9TgHdIWgl8m3Q2ssGaDOHekoj4dES8GDgB+ECv6+mEpOcBHweO63UtXXQp0B8RrwAuAxb1uJ66ppCGZvYm9XY/J2lqTyuqbx5wcUQ83etCajoUOC8iZgL7A1/Kv1MbpA228Da0eyuEC4FDxrSizo22L1sCuwNXSloBvBZYMoEvqo762kTEgxHxeH76eeC3x6m2TrTyXlsJLImIJyPiTuBnpLCfaNr5vZnHxB6Sgdb25yhgMUBEXA1sRrqp2AZpMoT7qLdCkFT95ToAuH0c62vHevclIh6OiGkR0R8R/aQLqgdFxGBvyh1VK6/NDpWnBwG3jmN97WrlthvfIPXakTSNNEyzfDyLbFFLtxCR9DJgG+Dqca6vXa3sz8+BfQAkvZwU7kPjWmUX9eyukOMlRrgVgqQPAYMRsQQ4VtKbgSeBNcD83lU8shb3ZYPR4v68R9JBwFPAQ6Rvz0xILe7P94C3SFoKPA28PyIe7F3VzbXxXpsHXBj5KyYTVYv7cxxpmOzvSBdXj5jo+7U+vv2AmVmBJsOwjJnZpONwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxA/w84Rpa0eJqlfQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KrtVypmSThdN","executionInfo":{"status":"ok","timestamp":1608185298601,"user_tz":480,"elapsed":331,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"outputId":"0b4f8867-b7da-483d-cf18-e7e319ecdf62"},"source":["# output the mean score & standard dev?\n","print(\"mean:\", np.mean(x))\n","print(\"standard dev:\", np.std(x))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mean: 0.49587628\n","standard dev: 0.1134721\n"],"name":"stdout"}]}]}